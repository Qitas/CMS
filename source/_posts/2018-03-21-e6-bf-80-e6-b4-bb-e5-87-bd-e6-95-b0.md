---
title: 激活函数
---

激活函数(Activation Function): **Sigmoid** | **Tanh** | **ReLU** **Sigmoid** 它能够把输入的连续实值“压缩”到0和1之间，如果是非常大的负数，那么输出就是0，如果是非常大的正数，输出就是1，主要缺点：当输入非常大或者非常小的时候，这些神经元的梯度是接近于0的；Sigmoid的输出不是0均值，这会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入。 **Tanh**是Sigmoid的变形，与 sigmoid 不同的是tanh 是0均值的，因此，实际应用中会比 sigmoid 更好。 **ReLU**相比于sigmoid和tanh，只需要一个阈值就可以得到激活值，而不用去算一大堆复杂的运算，ReLU 的缺点是训练的时候很容易就”die”，导致神经元的梯度永远都会是0。实际操作中设置了一个合适的较小的learning rate，可以降低这个问题发生的可能性。 激活函数(Activation Function)的特点： 非线性： 当激活函数是线性的时候，一个两层的神经网络就可以逼近基本上所有的函数 可微分： 当优化方法是基于梯度的时候，这个性质是必须的。 单调性： 当激活函数是单调的时候，单层网络能够保证是凸函数。 当激活函数满足这个性质的时候，如果参数的初始化是random的很小的值，那么神经网络的训练将会很高效。 当激活函数输出值是有限的时候，基于梯度的优化方法会更加稳定，因为特征的表示受有限权值的影响更显著；当激活函数的输出是无限的时候，模型的训练会更加高效，不过在这种情况小，一般需要更小的学习率。