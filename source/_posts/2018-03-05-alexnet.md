---
title: AlexNet
url: 1008.html
id: 1008
categories:
  - I·TQ
tags:
  - mAIn
---

2012年AlexNet，使用ReLU作为激活函数并发扬光大，效果超Sigmoid，解决后者梯度弥散。 有5个卷积层和3个全连接层，这8个训练层后设置ReLU，LRN层在前两个卷积层后，最大池化层在LRN层及最后一个卷积层后，AlexNet最后一层是1000类输出的Softmax层用于分类。 目前除了AlexNet，其他经典卷积神经网络都放弃了LRN，因为其牺牲了2/3的速度却没有明显的效果。