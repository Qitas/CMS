---
title: 神经网络
url: 915.html
id: 915
categories:
  - I·TQ
tags:
  - mAIn
---

##### 人工神经网络（ANNs: Artificial Neural Networks）:通过模仿动物神经网络行为，来处理信息的算法模型。神经网络（Neural Networks）的构想源自于我们对人类大脑的理解——神经元的彼此联系。二者不同之处在于人类大脑的神经元按特定的物理距离连接的，而人工神经网络有独立的层、连接，还有数据传播方向。

* * *

对人工神经网络的深入研究是深度学习发展的关键点。 早期的神经网络包括一个输入层(Input layer)、一个隐含层(Hidden layer)和一个输出层(Output layer)，也被称为感知机(perceptron)。 神经网络可以将输入的特征向量通过隐含层变换传递到输出层，在输出层可以得到分类后的结果。由于单层的隐含层过于简单，并没有多少实用价值，在80年代又诞生了拥有多个隐含层的多层感知机，也揭示了神经网络隐含层数决定其对物理世界的刻画能力。 当下火热的深度学习基础就是深度神经网络（Deep Neural Networks，简称DNN），所谓的深度就可以理解为包括很多的隐含层，可以对输入信息进行更复杂的处理，之后所涉及的神经网络都属于DNN。但有利有弊，神经网络随着层数的加深，优化函数容易陷入局部最优解偏离真正的全局最优，随着网络层数增加，“梯度消失”现象更加严重，低层基本上接受不到有效的训练信号而失去作用。高速公路网络（highway network）和深度残差学习网络（deep residual learning）用来避免梯度消失，已可以使层数达到一百多层。 根据隐含层处理信息的特点，可以将DNN再分为CNN和RNN。 卷积神经网络（Convolutional Neural Networks，简称CNN），通过输入层输入一个向量，然后给该向量的每一个元素分配权重，通过权重求和得到中间结果，然后将这个中间结果输入到激活函数，得到最后的输出结果。激活函数处理过程和人脑里面接受信息的过程类似，这个过程是非线性的，会滤掉部分信息，只关注感兴趣的地方。卷积网络结构对平移、比例缩放、倾斜或者共他形式的变形具有高度不变性，卷积层神经元只与前一层的部分神经元节点相连，相连的线对应一个权重w，我们将权重w构成的矩阵称为卷积核，大小由使用者定义，越大复杂度越高，刻画能力越强，深度学习偏向核小层多的结构。 卷积神经网络适合对静态的信息进行处理，如图像，但是如果输入到网络的信息会随时间变化且需要感知这种变化，例如声音和行为，CNN就不能实现时间顺序刻画了，于是有了RNN。 循环神经网络（Recurrent Neural Networks，简称RNN），可以看成增加时间维度的深度神经网络，它的循环体现在信息的传递方向不再是依次向下而包括回环，它的深度体现在时间长度上，“梯度消失”现象也会发生在时间轴上，梯度在时间轴上传播几层之后就会消失了，无法影响太遥远的过去，长短时记忆单元LSTM为解决时间上的梯度消失。