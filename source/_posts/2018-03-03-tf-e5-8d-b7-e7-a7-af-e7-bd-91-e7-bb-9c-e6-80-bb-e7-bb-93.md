---
title: TF卷积网络总结
url: 932.html
id: 932
categories:
  - I·TQ
tags:
  - mAIn
---

tensorflow学习案例MNIST数据集，需要给权重设置随机噪声打破完全对称，使用非线性激活函数ReLU避免死亡节点(dead neurons)，以前多使用Sigmoid函数。 使用Dropout层减轻过拟合，通过placeholder传入keepprob比率控制，让训练的时候随机丢弃一部分节点来减轻过拟合。 将dropout层输出连接到softmax层得到概率输出，使用的优化器是Adam。对激活函数的结果进行最大池化可以提升模型畸变容忍性。 学习案例CIFAR-10，对图像进行翻转镜像剪切等变换，制造了更多图片样本。通过数据增强可以给单幅图增加多个副本，提高图片利用率，避免参数众多的卷积网络过拟合，利用的正是图片的冗余特性增加不同噪声。 在最大池化层后使用LRN(局部响应归一化)层可以增加模型的泛化能力，目前流行的Trick还有批归一化层。 给weight增加L2正则化处理的loss，让特征权重不过大更平均，避免特征过多导致的过拟合，而L1正则处理可以让大部分特征权重置0消失。 限制隐含层节点的数量小于输入或输出节点，是一个降维过程，会去掉不重要特征，再给隐含层加L1正则，可以通过惩罚系数控制特征稀疏程度。 卷积层一般要搭配池化层，前面的卷积网络主要提取特征，最后的全连接层才对特征进行组合匹配和分类。全连接层基本上是使用一些矩阵乘法运算，而卷积层训练依赖cuDNN，算法更复杂。 卷积核的padding模式，SAME取样会超过边框且填充边界外的点，VALID取样不会超过边框。 通过tf.name_scope(xxx) as scope可以将内部变量自动命名并归于不同卷积层。