---
title: 估值网络
url: 1025.html
id: 1025
categories:
  - I·TQ
tags:
  - mAIn
---

学习行为对应期望价值的方法Q-Learning，从当前步到后续步总共可以期望获得的最大价值，然后选择Q值最大对应的行为。用来学习Q-Learning的网络模型就是估值网络，如果网络比较深，就是DQN。 通过在DQN中引入卷积层，可以让DQN具有一定的图形理解能力，如果DQN的输入是图像，那么前几层都会设置成卷积层。 深度学习需要大量的样本，通过Experience Replay技术随机抽取学习样本，保证大部分样本抽到的概率相近，可以避免只学习到最新的样本。 强化学习和Q-Learning学习目标一部分是自己输出的，频繁和大幅度更新模型参数会导致学习目标变化，DQN容易陷入目标Q和预测Q的反馈循环。可以使用一个稳定的target DQN辅助计算目标Q值，降低输出Q波动。 传统DQN会高估行为的Q值，且这种高估是不均匀的，可能导致次优Q值高于最优。通过主DQN选择行为后，让target DQN生成Q值的方式解决。 Dueling DQN将Q值函数拆为两部分，静态环境状态本身价值和行为带来的价值增量，这样可以使DQN的学习目标更稳定精确，对环境已状态估计能力更强。