---
title: 自然语言处理
url: 1914.html
id: 1914
comments: false
categories:
  - I·TQ
tags:
  - mAIn
---

_智慧通过沟通最能体现，人的面试如此，turing测试亦如此。_

* * *

声音实际上是一种波，有时间维度和强度维度，通过对时间维度切片，如每帧的长度为25毫秒，每两帧之间有25-10=15毫秒的交叠，称为以帧长25ms、帧移10ms分帧。 分帧后，语音就变成了很多小段。但波形在时域上几乎没有描述能力，因此必须将波形作变换。常见的一种变换方法是提取MFCC特征，根据人耳的生理特性，把每一帧波形变成一个多维向量，可以简单地理解为这个向量包含了这帧语音的内容信息（声学特征提取） 至此，声音就成了一个12行（假设声学特征是12维）、N列的一个矩阵，称之为观察序列，这里N为总帧数。观察序列如下图所示，图中，每一帧都用一个12维的向量表示，色块的颜色深浅表示向量值的大小。 ![](http://www.itq.ink/wp-content/uploads/2018/10/voice-300x153.png) 语音识别的步骤： 第一步，把帧识别成状态；可理解成比音素更细致的语音单位，通常把一个音素划分成3个状态。 第二步，把状态组合成音素；单词的发音由音素构成，对英语而言，一种常用的音素集是卡内基梅隆大学的一套由39个音素构成的音素集。 第三步，把音素组合成单词。

* * *

机器翻译是利用计算机把自然语言翻译成另一种自然语言的过程，基本流程分为三块：预处理、核心翻译、后处理： 预处理是对语言文字进行规整，把过长的句子通过标点符号分成几个短句子，过滤一些语气词和与意思无关的文字，将一些数字和表达不规范的地方，归整成符合规范的句子。 核心翻译模块是将输入的字符单元、序列翻译成目标语言序列的过程，这是机器翻译中最关键最核心的地方。 后处理模块是将翻译结果进行大小写的转化、建模单元进行拼接，特殊符号进行处理，使得翻译结果更加符合人们的阅读习惯。

* * *

文本类深度学习步骤 第一步：词向量 词向量表将高维的稀疏二值向量映射成低维的稠密向量。 大部分神经网络模型首先都会把输入文本切分成若干个词语，然后将词语都用词向量表示。另一些模型用其它信息扩展了词向量表示。比如，除了词语的ID之外，还会输入一串标签。然后可以学习得到标签向量，将标签向量拼接为词向量。这可以让你将一些位置敏感的信息加入到词向量表示中。然而，有一个更强大的方式来使词语表示呈现出语境相关。 第二步：编码 假设得到了词向量的序列，编码这一步是将其转化为句子矩阵，矩阵的每一行表示每个词在上下文中所表达的意思。 这一步用到了双向RNN模型。LSTM和GRU结构的模型效果都不错。每一行向量通过两部分计算得到：第一部分是正向计算，第二部分是逆向计算，然后拼接两部分得到完整的向量。 第三步：注意力机制 这一步是将上一步的矩阵表示压缩为一个向量表示，因此可以被送入标准的前馈神经网络进行预测。 Yang等人在2016年发表的论文提出了一种注意力机制，输入一个矩阵，输出一个向量。区别于从输入内容中提取一个上下文向量，该机制的上下文向量是被当做模型的参数学习得到。这使得注意机制变成一个纯粹的压缩操作，可以替换任何的池化步骤。 第四步：预测 文本内容被压缩成一个向量之后，我们可以学习最终的目标表达：一种类别标签、一个实数值或是一个向量等等。我们也可以将网络模型看做是状态机的控制器，如一个基于转移的解析器，来做结构化预测。 大部分的NLP模型通常更青睐浅层的前馈网络，这意味着在机器视觉领域取得的重要技术至今为止并没有影响到NLP领域，比如residual connections 和 batch normalization。

* * *